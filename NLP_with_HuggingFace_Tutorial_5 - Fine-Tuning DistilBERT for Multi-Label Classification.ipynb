{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Description</th>\n",
       "      <th>Director</th>\n",
       "      <th>Actors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Runtime (Minutes)</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Revenue (Millions)</th>\n",
       "      <th>Metascore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Guardians of the Galaxy</td>\n",
       "      <td>Action,Adventure,Sci-Fi</td>\n",
       "      <td>A group of intergalactic criminals are forced ...</td>\n",
       "      <td>James Gunn</td>\n",
       "      <td>Chris Pratt, Vin Diesel, Bradley Cooper, Zoe S...</td>\n",
       "      <td>2014</td>\n",
       "      <td>121</td>\n",
       "      <td>8.1</td>\n",
       "      <td>757074</td>\n",
       "      <td>333.13</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Prometheus</td>\n",
       "      <td>Adventure,Mystery,Sci-Fi</td>\n",
       "      <td>Following clues to the origin of mankind, a te...</td>\n",
       "      <td>Ridley Scott</td>\n",
       "      <td>Noomi Rapace, Logan Marshall-Green, Michael Fa...</td>\n",
       "      <td>2012</td>\n",
       "      <td>124</td>\n",
       "      <td>7.0</td>\n",
       "      <td>485820</td>\n",
       "      <td>126.46</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Split</td>\n",
       "      <td>Horror,Thriller</td>\n",
       "      <td>Three girls are kidnapped by a man with a diag...</td>\n",
       "      <td>M. Night Shyamalan</td>\n",
       "      <td>James McAvoy, Anya Taylor-Joy, Haley Lu Richar...</td>\n",
       "      <td>2016</td>\n",
       "      <td>117</td>\n",
       "      <td>7.3</td>\n",
       "      <td>157606</td>\n",
       "      <td>138.12</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sing</td>\n",
       "      <td>Animation,Comedy,Family</td>\n",
       "      <td>In a city of humanoid animals, a hustling thea...</td>\n",
       "      <td>Christophe Lourdelet</td>\n",
       "      <td>Matthew McConaughey,Reese Witherspoon, Seth Ma...</td>\n",
       "      <td>2016</td>\n",
       "      <td>108</td>\n",
       "      <td>7.2</td>\n",
       "      <td>60545</td>\n",
       "      <td>270.32</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Suicide Squad</td>\n",
       "      <td>Action,Adventure,Fantasy</td>\n",
       "      <td>A secret government agency recruits some of th...</td>\n",
       "      <td>David Ayer</td>\n",
       "      <td>Will Smith, Jared Leto, Margot Robbie, Viola D...</td>\n",
       "      <td>2016</td>\n",
       "      <td>123</td>\n",
       "      <td>6.2</td>\n",
       "      <td>393727</td>\n",
       "      <td>325.02</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                    Title                     Genre  \\\n",
       "0     1  Guardians of the Galaxy   Action,Adventure,Sci-Fi   \n",
       "1     2               Prometheus  Adventure,Mystery,Sci-Fi   \n",
       "2     3                    Split           Horror,Thriller   \n",
       "3     4                     Sing   Animation,Comedy,Family   \n",
       "4     5            Suicide Squad  Action,Adventure,Fantasy   \n",
       "\n",
       "                                         Description              Director  \\\n",
       "0  A group of intergalactic criminals are forced ...            James Gunn   \n",
       "1  Following clues to the origin of mankind, a te...          Ridley Scott   \n",
       "2  Three girls are kidnapped by a man with a diag...    M. Night Shyamalan   \n",
       "3  In a city of humanoid animals, a hustling thea...  Christophe Lourdelet   \n",
       "4  A secret government agency recruits some of th...            David Ayer   \n",
       "\n",
       "                                              Actors  Year  Runtime (Minutes)  \\\n",
       "0  Chris Pratt, Vin Diesel, Bradley Cooper, Zoe S...  2014                121   \n",
       "1  Noomi Rapace, Logan Marshall-Green, Michael Fa...  2012                124   \n",
       "2  James McAvoy, Anya Taylor-Joy, Haley Lu Richar...  2016                117   \n",
       "3  Matthew McConaughey,Reese Witherspoon, Seth Ma...  2016                108   \n",
       "4  Will Smith, Jared Leto, Margot Robbie, Viola D...  2016                123   \n",
       "\n",
       "   Rating   Votes  Revenue (Millions)  Metascore  \n",
       "0     8.1  757074              333.13       76.0  \n",
       "1     7.0  485820              126.46       65.0  \n",
       "2     7.3  157606              138.12       62.0  \n",
       "3     7.2   60545              270.32       59.0  \n",
       "4     6.2  393727              325.02       40.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Movie-Data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Description', 'Genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Drama        513\n",
       "Action       303\n",
       "Comedy       279\n",
       "Adventure    259\n",
       "Thriller     195\n",
       "Crime        150\n",
       "Romance      141\n",
       "Sci-Fi       120\n",
       "Horror       119\n",
       "Mystery      106\n",
       "Fantasy      101\n",
       "Biography     81\n",
       "Family        51\n",
       "Animation     49\n",
       "History       29\n",
       "Sport         18\n",
       "Music         16\n",
       "War           13\n",
       "Western        7\n",
       "Musical        5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do data analysis\n",
    "df['Genre'].value_counts()\n",
    "\n",
    "# individual genre counts\n",
    "genre = df['Genre'].tolist()\n",
    "\n",
    "genre = [x for g in genre for x in g.split(',')]\n",
    "\n",
    "genre_counts = pd.Series(genre).value_counts()\n",
    "\n",
    "genre_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Description analysis\n",
    "df['Description'].str.len().plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Genre'] = df['Genre'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Label Encoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multilabel = MultiLabelBinarizer()\n",
    "\n",
    "labels = multilabel.fit_transform(df['Genre']).astype('float32')\n",
    "\n",
    "texts = df['Description'].tolist()\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch and transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genre_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "chekpoint = 'distilbert-base-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(chekpoint)\n",
    "# model = AutoModel.from_pretrained(chekpoint, num_labels=len(genre_counts))\n",
    "\n",
    "# Load DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input texts\n",
    "tokenized_input = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Prepare PyTorch dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx][0])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(tokenized_input, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1037,  2177,  1997,  6970,  9692, 28804, 12290,  2024,  3140,\n",
       "          2000,  2147,  2362,  2000,  2644,  1037,  5470, 12070,  2389,  6750,\n",
       "          2013,  2635,  2491,  1997,  1996,  5304,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1, dtype=torch.int32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x['input_ids'].shape[0] for x in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Validation Loss: 0.3446393430233002\n",
      "Epoch 2/3, Validation Loss: 0.3066943979263306\n",
      "Epoch 3/3, Validation Loss: 0.2780289214849472\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Sample data (replace with your own dataset)\n",
    "# texts = [\"This is text 1.\", \"Another example here.\", \"More text for the third sample.\"]\n",
    "# labels = [[1, 0, 1], [0, 1, 0], [1, 1, 1]]  # Multi-label format\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels[0]), problem_type='multi_label_classification')\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, hamming_loss, jaccard_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "# Custom multi-label classification loss\n",
    "def multi_label_loss(output, target):\n",
    "    return F.binary_cross_entropy_with_logits(output, target.float())\n",
    "\n",
    "# Training parameters\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in DataLoader(train_dataset, batch_size=8, shuffle=True):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        loss = multi_label_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for batch in DataLoader(val_dataset, batch_size=8):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        val_loss = multi_label_loss(outputs, labels)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    average_val_loss = sum(val_losses) / len(val_losses)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {average_val_loss}')\n",
    "\n",
    "# You can further add evaluation metrics and save the trained model as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2791, 0.3293, 0.1239, 0.1380, 0.3181, 0.2046, 0.5374, 0.1152, 0.1411,\n",
       "         0.0856, 0.1907, 0.0824, 0.0601, 0.1763, 0.1846, 0.2205, 0.0815, 0.2279,\n",
       "         0.0822, 0.0778],\n",
       "        [0.2345, 0.3194, 0.1433, 0.1689, 0.3466, 0.2217, 0.5745, 0.1354, 0.1763,\n",
       "         0.1114, 0.2108, 0.1100, 0.0784, 0.1989, 0.2410, 0.2184, 0.1047, 0.2502,\n",
       "         0.1051, 0.0918],\n",
       "        [0.1864, 0.2165, 0.1034, 0.1324, 0.2885, 0.1936, 0.5940, 0.0862, 0.1231,\n",
       "         0.0761, 0.1806, 0.0716, 0.0505, 0.1672, 0.2136, 0.1810, 0.0663, 0.2276,\n",
       "         0.0649, 0.0578]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do classification on sample data\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# classify on sample data\n",
    "tokenized_input = tokenizer([\"This is text 1.\", \"Another example here.\", \"More text for the third sample.\"], truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "\n",
    "outputs = model(**tokenized_input)\n",
    "\n",
    "# get the prediction\n",
    "predictions = torch.sigmoid(outputs.logits)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79dc7620e5c4d3b9c5d27c6cdf9f34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 29.6157, 'train_samples_per_second': 81.038, 'train_steps_per_second': 10.13, 'train_loss': 0.31370747884114586, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0aec69b6134511b5765c0e7a281c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2691498100757599, 'eval_roc_auc': 0.5894200259814462, 'eval_hamming_loss': 0.1145, 'eval_f1': 0.2339616725539116, 'eval_runtime': 0.8932, 'eval_samples_per_second': 223.922, 'eval_steps_per_second': 27.99, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels[0]), problem_type='multi_label_classification')\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, hamming_loss, jaccard_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "def multi_labels_metrics(predictions, labels, threshold=0.3):\n",
    "    # first, apply sigmoid on predictions of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "\n",
    "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'macro')\n",
    "    # accuracy = accuracy_score(y_true, y_pred)\n",
    "    hamming = hamming_loss(y_true, y_pred)\n",
    "    # jaccard = jaccard_score(y_true, y_pred, average = 'macro')\n",
    "\n",
    "    # return as dictionary\n",
    "    metrics = {'roc_auc': roc_auc,\n",
    "               'hamming_loss': hamming,\n",
    "               'f1': f1\n",
    "               }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # If type(p.predictions) = tuple, we take p.predictions[0]; otherwise p.predictions\n",
    "    preds = p.predictions[0] if isinstance(p.predictions,\n",
    "            tuple) else p.predictions\n",
    "    result = multi_labels_metrics(\n",
    "        predictions=preds,\n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(results)\n",
    "\n",
    "# save model\n",
    "trainer.save_model(\"distilbert-finetuned-sem_eval-english-weights\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd050d24da84eefb630ab96e2016a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2691498100757599,\n",
       " 'eval_roc_auc': 0.5894200259814462,\n",
       " 'eval_hamming_loss': 0.1145,\n",
       " 'eval_f1': 0.2339616725539116,\n",
       " 'eval_runtime': 0.9686,\n",
       " 'eval_samples_per_second': 206.482,\n",
       " 'eval_steps_per_second': 25.81,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a235d888204a428f8f04614cf27036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2691498100757599,\n",
       " 'eval_roc_auc': 0.5463089080104132,\n",
       " 'eval_hamming_loss': 0.10175,\n",
       " 'eval_f1': 0.12918201095548074,\n",
       " 'eval_runtime': 0.854,\n",
       " 'eval_samples_per_second': 234.193,\n",
       " 'eval_steps_per_second': 29.274,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9234,  0.5812, -2.4778, -2.8435, -1.5859, -2.3758, -1.3815, -2.4439,\n",
       "         -1.7346, -3.1692, -1.6619, -3.2255, -3.8561, -2.1152, -2.6588, -0.5707,\n",
       "         -3.2460, -1.6175, -3.3584, -3.2835]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "text = \"Following clues to the origin of mankind, a team finds a structure on a distant moon, but they soon realize they are not alone.\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding.to(trainer.model.device)\n",
    "\n",
    "outputs = trainer.model(**encoding)\n",
    "\n",
    "# The logits is a tensor that contains the (unnormalized) scores for every individual label.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To turn them into actual predicted labels, we first apply a sigmoid function independently to every score, \n",
    "# such that every score is turned into a number between 0 and 1, that can be interpreted as a \"probability\" \n",
    "# for how certain the model is that a given class belongs to the input text.\n",
    "\n",
    "# Next, we use a threshold (typically, 0.5) to turn every probability into either a 1 \n",
    "# (which means, we predict the label for the given example) or a 0 (which means, we don't predict the label for the given example).\n",
    "\n",
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(outputs.logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.3)] = 1\n",
    "\n",
    "# turn predicted id's into actual label names\n",
    "# predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label > 0]\n",
    "# print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime',\n",
       "       'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music',\n",
       "       'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Sport', 'Thriller',\n",
       "       'War', 'Western'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Action', 'Adventure', 'Sci-Fi')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel.inverse_transform(predictions.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
